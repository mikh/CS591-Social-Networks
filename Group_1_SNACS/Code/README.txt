
	Social Networks Analysis using Compressive Sensing (SNACS)

==============================================================================

Supported System:

This code package has been tested and verified for Windows 10 64-bit machine running Python 3.5 and Matlab 2013. The Matlab code will run on nearly any matlab installation regardless of operating system. The Python code will likely work on Python installation 3.4+ and on Windows 7+. There is no support for a non-Windows environment for the Python network.


=================

Installation:

There is no installation required for the MATLAB code.

To install the Python code, first ensure you are running Python 3.5. From there run the install.bat (or install.sh if you have a bash shell). This will create a virtual environment for Python to run from, and install all appropriate packages used by Python. The path from which to run the python scripts is:
	py\Scripts\python


=================

Package structure

code_package\
	config\
	data\
		modules\
			reddit_crawler\
				storage\
				raw_results\
				graphs\
				checked_subreddits.json
				subreddits.txt
				users.json
	lib\
	logs\
	py\
	whls\
	install.bat
	install.sh
	plotly_visualization.py
	networkx_visualization.py
	README.txt
	reddit_crawler.py
	visualization.py
	raw_parser.py
	matrix_generator.py

config\ = contains configuration data for Python scripts
data\modules\reddit_crawler\ = data used by reddit data miner
	storage\ = contains files of user information
	raw_results\ = raw data output from algorithm
	graphs\ = processed graph data ready for visualization
	checked_subreddits.json = subreddits that have already been crawled
	subreddits.txt = all subreddits to crawl
	users.json = current list of users
lib\ = library python files containing functions run by the scripts
logs\ = log files for python scripts
py\ = Python interpreter
whls\ = installation files
install.bat = batch installation file
install.sh = bash installation file
plotly_visualization.py = source file used by visualization.py to provide plotly support
networkx_visualization.py = source file used by visualization.py to provide networkx support
README.txt = this document
reddit_crawler.py = Crawler script to extract data from reddit. See later for options
visualization.py = Visualization script to display data in a chart
raw_parser.py = Converts raw data output of matlab to visualization ready data
matrix_generator.py = Script that creates an Adjacency matrix


=================

reddit_crawler.py

The reddit crawler has several flags to determine functionality:

py\Scripts\python reddit_crawler.py --crawl_for_users 
	This will obtain the top 10 submissions of a subreddit, then process all the comments in the submission to acquire a list of users.

py\Scripts\python reddit_crawler.py --combine_JSON_files
	This will combine all files in the data\modules\reddit_crawler\storage\ folder into a single data\modules\reddit_crawler\users.json. This functionality is not recommeneded as the amount of data available is very large and will make it difficult to use. Use the appropriate script provided to generate a graph.

py\Scripts\python reddit_crawler.py --generate_unprocessed_user_list
	This generates a list of unprocessed users. Makes user-processing much faster. Run this after you have finished crawling for users, but before you process the users

py\Scripts\python reddit_crawler.py --process_users
	This will process the users in the users.json file and acquire all of the subreddits they comment in. These listings are then grouped with the user, and stored in the storage\ folder. This will later be used to generate a graph.

py\Scripts\python reddit_crawler.py --generate_graph
	This will generate a graph from the users.json file. This function is not recommended as the amount of data is very large, and the graph generation is a brute-force method. Please see the later script for graph generation.

To properly crawl for data please run these commands in order:

py\Scripts\python reddit_crawler.py --crawl_for_users
py\Scripts\python reddit_crawler.py --generate_unprocessed_user_list
py\Scripts\python reddit_crawler.py --process_users

This will add data to storage\ which is later used to generate a graph. All data already present there has been generated using this method.


=================

raw_parser.py

This will convert some early raw data generated by the algorithm into a visualizable result. This script should not have to be run.

py\Scripts\python raw_parser.py --set_file=<file> --parse<num>
	Processes <file> using parse<num> method and outputs the result to the graph directory.



=================

visualization.py

Loads graph data and creates a visualization of the data. To run, specify the folder in graphs\ to load data from. From there it will create all appropriate graphs

py\Scripts\python visualization.py --set_graph=<folder> --plotly
	Loads the graph represented by <folder> using the plotly method


=================

matrix_generator.py

Converts data files from crawler into an adjacency matrix. To run specify the number of groups to have, the total number of nodes to use, and the name of the output file

py\Scripts\python matrix_generator.py --set_groups=<num_groups> --set_nodes=<num_nodes> --set_name=<name_of_output_file>
	Converts the raw data into an adjacency matrix. There are other settings here as well, but they have been optimized for fastest and most accurate output.

	